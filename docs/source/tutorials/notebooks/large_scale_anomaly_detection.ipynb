{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale Anomaly Detection\n",
    "\n",
    "**Goal:** In this tutorial, we will explore how to perform anomaly detection on a large-scale graph comprising more than 49 million edges.\n",
    "We will notably see how to handle this large graph by dividing it into small sequential temporal snapshots that fit into memory, and by reducing even more their size with edge compression.\n",
    "\n",
    "**Concepts:** `Anomaly Detection`, `Unsupervised Learning`, `Edge-based MPNN`, `Lazy Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into this tutorial, you will first need to install the following additional dependencies.\n",
    "\n",
    "`pip install matplotlib scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "from mlx_graphs.datasets import LANLDataset\n",
    "from mlx_graphs.loaders import LANLDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using here the `LANL` dataset. It is made up of 58 consecutive days of data gathered from\n",
    "the Los Alamos National Laboratory’s internal\n",
    "computer network. This version of the dataset comprises **49,341,086 auth events**\n",
    "from a typical APT campaign across **17,685 Windows machines**, including\n",
    "**705 malicious events**. Graphs can be constructed from authentication events,\n",
    "where an edge represents an authentication action and a node represents\n",
    "a machine within the network.\n",
    "Each edge is associated with either a **benign** or **malicious** label, making this\n",
    "dataset suitable for edge prediction/detection tasks, as well as node-based\n",
    "tasks, where the machine at the origin of malicious activity can be identified.\n",
    "\n",
    "The version of the LANL dataset proposed within mlx-graphs is already\n",
    "preprocessed to only include data required to build the graphs along with\n",
    "**2 default edge features** extracted from the raw dataset. Consequently,\n",
    "this class will by default download a 340MB archive instead of the original\n",
    "files that represent more than 7GB compressed.\n",
    "\n",
    "The 2 provided edge features are:\n",
    "\n",
    "* `success/failure`: 1 if the authentication succeeded, 0 if it failed\n",
    "* `logon type`: identifies the type of the source user that initiated the\n",
    "authentication (user -> 1, computer -> 2, anonymous -> 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating an instance of the LANL dataset is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset = LANLDataset(process_original_files=False, use_gzip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large size of this dataset, loading it directly into memory can quickly become hard task. For this reason, the `LANLDataset` class is known as a `LazyDataset`. In mlx-graphs, a `LazyDataset` is a dataset that builds and loads graphs into memory only when requested.\n",
    "When first instantiated, the lazy dataset downloads all required files from the Internet and stores them on disk. Typically, the large dataset is divided into multiple files that can be easily loaded independently on memory. In the case of LANL, a timestamp is by default associated to every authentication events, this allows to divide the large graph into sequential graph snapshots, lasting for a given period of time. Concretely, any dataset comprising a temporal indicator for every edge can be similarly divided into a sequence of temporal snapshots.\n",
    "\n",
    "In the current implementation of `LANLDataset`, each file comprises 1 minute of authentication events. To build and load a graph at a particular time, indexing can be used directly on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 158), int64)\n",
       "\tnode_features(shape=(17685, 17685), float32)\n",
       "\tedge_features(shape=(158, 4), float32)\n",
       "\tedge_labels(shape=(158,), int64)\n",
       "\tedge_timestamps(shape=(158,), int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph basically represents the first minute of data in the dataset.\n",
    "\n",
    "Requesting a slice from the dataset results in merging all the graphs from this range into a single graph. For example, loading the first day of data can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 688294), int64)\n",
       "\tnode_features(shape=(17685, 17685), float32)\n",
       "\tedge_features(shape=(688294, 4), float32)\n",
       "\tedge_labels(shape=(688294,), int64)\n",
       "\tedge_timestamps(shape=(688294,), int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_day_in_minutes = 60 * 24\n",
    "dataset[:one_day_in_minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "To ease the consistent iteration over the entire dataset, we made available `LANLDataLoader`, a data loader dedicated to the LANL dataset.\n",
    "\n",
    "The `batch_size` argument defines the duration of a snapshot graph, in minutes. `split` denotes the portion of the dataset to iterate on (\"all\" | \"train\" | \"valid\" | \"test\"). The `compress_edges` indicates whether to use edge compression or not.\n",
    "Edge compression means that for a single requested graph, all duplicate edges between any two nodes will be compressed into a unique edge, with additional features based on the statistics of the number of compressed edges. This effectively reduces the size of the graph by eliminating duplicate edges, which may be many as the size of the graph expends. In this tutorial, we will be using edge compression.\n",
    "\n",
    "Using edge compression, the 2 initial edge features become 6 edge features:\n",
    "\n",
    "* `#edges`: the number of duplicate edges in the compressed edge\n",
    "* `#successes`: the number of success authentications\n",
    "* `#failures`: the number of failure authentications\n",
    "* `#src_type_user`: the number of \"user\" source node type\n",
    "* `#src_type_computer`: the number of \"computer\" source node type\n",
    "* `#src_type_anonymous`: the number of \"anonymous\" source node type\n",
    "\n",
    "Now that we know the theory behind the data loader, let's iterate on it and compute some statistics about the compressed graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNAPSHOT_DURATION = 60  # in minutes\n",
    "loader = LANLDataLoader(dataset, split=\"all\", compress_edges=True, batch_size=SNAPSHOT_DURATION, remove_self_loops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = next(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the first iteration over a loader actually computes all the graph snapshots and save them on disk for a much faster loading on the subsequent iterations. The first iteration is thus pretty long as millions of edges have to be pre-processed from the raw downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [03:20<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from mlx_graphs.utils import degree\n",
    "\n",
    "num_edges, num_nodes, in_degrees, out_degrees = 0, 0, 0, 0\n",
    "num_labels, num_snapshots_with_labels = 0, 0\n",
    "\n",
    "for g in loader:\n",
    "    num_edges += g.num_edges\n",
    "    num_nodes = int(g.num_nodes)\n",
    "    in_degrees += degree(g.edge_index[1]).mean().item()\n",
    "    out_degrees += degree(g.edge_index[0]).mean().item()\n",
    "\n",
    "    labels = g.edge_labels.sum()\n",
    "    num_labels += labels\n",
    "    \n",
    "    if labels > 0:\n",
    "        num_snapshots_with_labels += 1\n",
    "\n",
    "mean_num_edges = num_edges / len(loader)\n",
    "in_degrees /= len(loader)\n",
    "out_degrees /= len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANL statistics (with 60 min snapshot duration + edge compression)\n",
      "======================================================================\n",
      "Number of graph snapshots: 1392\n",
      "Number of nodes: 17685\n",
      "Total number of edges: 8077\n",
      "Avg number of edges per graph: 11244558.00\n",
      "Avg node in-degree: 0.61\n",
      "Avg node out-degree: 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f\"LANL statistics (with {SNAPSHOT_DURATION} min snapshot duration + edge compression)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Number of graph snapshots: {len(loader)}\")\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Total number of edges: {int(num_edges)}\")\n",
    "print(f\"Avg number of edges per graph: {mean_num_edges:.2f}\")\n",
    "print(f\"Avg node in-degree: {in_degrees:.2f}\")\n",
    "print(f\"Avg node out-degree: {out_degrees:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, subsequent iterations on the loader become much faster as they are directly loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1392 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [00:00<00:00, 1824.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for g in loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Our goal is to design a GNN-based model able to differentiate between benign and malicious authentication events, with the additional constraint that we do not want to leverage any label in the training. In other terms, we're looking to perform **unsupervised edge detection**.\n",
    "\n",
    "### GNN Encoder\n",
    "Our proposed model is simple and aims to show how to merge pre-defined GNN layers like the `GCNConv`, with a custom MPNN integrating edge features.\n",
    "\n",
    "The GCN layer can be defined as:\n",
    "$$\n",
    "H = \\sigma \\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A}\\tilde{D}^{\\frac{-1}{2}} H W \\right),\n",
    "$$\n",
    "where $H$ denotes the node embeddings at layer $l$ and $H = X$ at the first layer, with $X$ the initial node features matrix. $\\tilde{A}$ is the adjacency matrix representation of the graph with self-loops such that $\\tilde{A}=A+I$, with $I$ the identity matrix of same shape as the adjacency matrix $A$. $\\tilde{D}$ is the degree matrix of $\\tilde{A}$, whereas $W$ represents a trainable weight matrix and $\\sigma$ is the sigmoid non-linear activation function. Please note that in this context, we also utilize `edge_weights` in every call to the GCN layer. This feature is provided by the first edge feature, which is the number of duplicate edges. It has been standardized on a batch-wise basis, ensuring that the weight falls within the range of 0 to 1. Consequently, it can function as an edge weight, allowing the exchange of information between nodes in proportion to the number of duplicate edges within a compressed edge.\n",
    "\n",
    "We also introduce a custom MPNN layer that simply integrates the edge features into the node embeddings:\n",
    "$$\n",
    "H_u = \\sigma\\left( H_u + \\sum_{v \\in \\mathcal{N}(u)} H_u W_{\\text{edge}}(E_{uv}) \\right)\n",
    "$$\n",
    "where $E_{uv}$ represents the feature vector of the edge $(u,v)$, $\\mathcal{N}(u)$ denotes the neighbors of node $u$ and $W_{\\text{edge}}$ is a weight matrix to project the edge features into a dimension that matches the shape of node embeddings.\n",
    "\n",
    "Finally, edge embeddings can be derived from the node embeddings:\n",
    "$$\n",
    "H_{uv} = W_{proj}\\left( H_u + H_v\\right), \\quad (u, v) \\in \\mathcal{E}\n",
    "$$\n",
    "\n",
    "### Decoder\n",
    "\n",
    "We want here to train the edge embeddings in an unsupervised manner. We propose an encoder/decoder architecture, where we compress the edge embeddings into a low-dimensional space, and decode this compressed representation to preserve the structural and integrated features captured by the encoder. Formally, the reconstructed estimation of an edge embedding $H_{uv}$ can be defined as:\n",
    "$$\n",
    "\\hat{H}_{uv} = \\text{AE}(H_{uv})\n",
    "$$\n",
    "where $\\text{AE}$ denotes an Autoencoder model.\n",
    "\n",
    "Trained on benign edges, this encoder will learn to generate low reconstruction errors for seen benign edges, aiming to produce high reconstruction errors for anomalous edges during testing.\n",
    "The reconstruction error between the edge embedding $H_{uv}$ and the reconstructed edge embedding $\\hat{H}_{uv}$ is calculated with the MSE loss: \n",
    "\n",
    "$$\n",
    "e_{uv} = (H_{uv} - \\hat{H}_{uv})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn\n",
    "from mlx_graphs.nn import GCNConv, MessagePassing\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim=None):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim if out_dim is None else out_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        encoded = nn.relu(self.encoder(x))\n",
    "        decoded = nn.sigmoid(self.decoder(encoded))\n",
    "        return decoded\n",
    "\n",
    "class GRU_Argus(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, hidden_units=1):\n",
    "        super(GRU_Argus, self).__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            x_dim, h_dim,\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.lin = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def __call__(self, xs, h0, include_h=False):\n",
    "        xs = self.drop(xs)\n",
    "\n",
    "        if isinstance(h0, type(None)):\n",
    "            xs = self.rnn(xs)\n",
    "        else:\n",
    "            xs, h = self.rnn(xs, h0)\n",
    "\n",
    "        if not include_h:\n",
    "            return self.lin(xs)\n",
    "\n",
    "        return self.lin(xs), h\n",
    "\n",
    "class EdgeGCN(MessagePassing):\n",
    "    \"\"\"\n",
    "    A simple GNN model that with 2 GCN layers and a custom MPNN layer integrating\n",
    "    edge features with sum aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features_dim: int,\n",
    "        hidden_features_dim: int,\n",
    "        out_features_dim: int,\n",
    "        edge_features_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.25,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "        layer_sizes = (\n",
    "            [node_features_dim]\n",
    "            + [hidden_features_dim] * (num_layers - 1)\n",
    "            + [out_features_dim]\n",
    "        )\n",
    "        self.gcn_layers = [\n",
    "            GCNConv(in_dim, out_features_dim, bias, add_self_loops=True)\n",
    "            for in_dim, out_features_dim in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.W_edge = nn.Linear(edge_features_dim, out_features_dim)\n",
    "        self.W_proj = nn.Linear(out_features_dim, out_features_dim)\n",
    "\n",
    "        self.AE = Autoencoder(out_features_dim, out_features_dim // 2)\n",
    "        self.gru = GRU_Argus(out_features_dim, hidden_features_dim, out_features_dim)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        edge_index: mx.array,\n",
    "        node_features: mx.array,\n",
    "        edge_weights: mx.array,\n",
    "        edge_features: mx.array,\n",
    "    ) -> mx.array:\n",
    "        \"\"\"Calculates the overall loss for training\"\"\"\n",
    "        # First computes the reconstruction error of every edges (i.e. e_{uv}, for all (u, v))\n",
    "        H = self._forward(edge_index, node_features, edge_weights, edge_features)\n",
    "        \n",
    "        # Computes reconstruction estimation \\hat{H}_{uv}\n",
    "        H_hat = self.AE(H)\n",
    "\n",
    "        # Computes the MSE loss for all edges, with mean reduction, (i.e. mean of all e_{uv})\n",
    "        loss = nn.losses.mse_loss(H_hat, H, reduction=\"mean\")\n",
    "        return loss\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        edge_index: mx.array,\n",
    "        node_features: mx.array,\n",
    "        edge_weights: mx.array,\n",
    "        edge_features: mx.array,\n",
    "    ) -> mx.array:\n",
    "        \"\"\"Calculates edge anomaly scores\"\"\"\n",
    "        # First computes the reconstruction error of every edges (i.e. e_{uv}, for all (u, v))\n",
    "        H = self._forward(edge_index, node_features, edge_weights, edge_features)\n",
    "        \n",
    "        # Computes reconstruction estimation \\hat{H}_{uv}\n",
    "        H_hat = self.AE(H)\n",
    "        \n",
    "        # Computes MSE loss for each edge (all e_{uv} without reduction)\n",
    "        # This will serve as the anomalous score\n",
    "        recon_scores = nn.losses.mse_loss(H_hat, H, reduction=\"none\")\n",
    "        recon_scores = mx.sum(recon_scores, axis=1)\n",
    "        return recon_scores\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        edge_index: mx.array,\n",
    "        node_features: mx.array,\n",
    "        edge_weights: mx.array,\n",
    "        edge_features: mx.array,\n",
    "    ):\n",
    "        \"\"\"Simply computes edge embeddings\"\"\"\n",
    "        # Computes node embeddings with GCN layers\n",
    "        for layer in self.gcn_layers[:-1]:\n",
    "            H = nn.tanh(layer(edge_index, node_features, edge_weights=edge_weights))\n",
    "            H = self.dropout(H)\n",
    "        H = self.gcn_layers[-1](edge_index, H, edge_weights=edge_weights)\n",
    "\n",
    "        H = self.gru(H, None)\n",
    "\n",
    "        # Integrates edge features with the custom MPNN defined in `message()`\n",
    "        # the sum aggregation happens with `super().__init__(aggr='add')`\n",
    "        aggreg = self.propagate(edge_index, H, message_kwargs={\"edge_features\": edge_features})\n",
    "        H = nn.tanh(H + aggreg)\n",
    "\n",
    "        # Computes edge embeddings with sum of the two end nodes\n",
    "        H = self.W_proj(H[edge_index[0, :]] + H[edge_index[1, :]])\n",
    "        return H\n",
    "\n",
    "\n",
    "    def message(self, src_features, dst_features, edge_features):\n",
    "        \"\"\"Computes edge messages to integrate edge feature information\"\"\"\n",
    "        # Applies a linear transformation to match the node and edge embedding shapes\n",
    "        edge_features = self.W_edge(edge_features)\n",
    "\n",
    "        # Integrates transformed edge features into the node emeddings\n",
    "        return src_features * edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 17685\n",
    "HIDDEN_SIZE = 64\n",
    "OUT_SIZE = 32\n",
    "EDGE_FEATURES_DIM = 6\n",
    "NUM_LAYERS = 2\n",
    "LR = 0.0001\n",
    "\n",
    "model = EdgeGCN(\n",
    "    node_features_dim=NUM_NODES,\n",
    "    hidden_features_dim=HIDDEN_SIZE,\n",
    "    out_features_dim=OUT_SIZE,\n",
    "    edge_features_dim=EDGE_FEATURES_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bias=True,\n",
    ")\n",
    "mx.eval(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will be using the `Adam` optimizer provided in MLX, and we also define a `forward_fn` function that is passed to `value_and_grad` to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.optimizers as optim\n",
    "\n",
    "def forward_fn(model, edge_index, node_features, edge_weights, edge_features, labels, inference=False):\n",
    "    if inference:\n",
    "        return model.inference(edge_index, node_features, edge_weights, edge_features)\n",
    "    return model(edge_index, node_features, edge_weights, edge_features)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=LR)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LANLDataLoader` comes with pre-defined time ranges to split the dataset into train/eval/test sets. We'll use those ranges as an example.\n",
    "The train and eval loaders contain benign-only graphs built from benign authentication activity in the LANL network, whereas the test set comprises some malicious events from APT campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/39 [01:00<38:04, 60.12s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = LANLDataLoader(dataset, split=\"train\", compress_edges=True, batch_size=SNAPSHOT_DURATION, tqdm_bar=True)\n",
    "eval_loader = LANLDataLoader(dataset, split=\"valid\", compress_edges=True, batch_size=SNAPSHOT_DURATION, tqdm_bar=True)\n",
    "test_loader = LANLDataLoader(dataset, split=\"test\", compress_edges=True, batch_size=SNAPSHOT_DURATION, tqdm_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a dedicated function to either train, eval or test the model for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    avg_loss = 0.\n",
    "    for graph in train_loader:\n",
    "        edge_weights = graph.edge_features[:, 0]\n",
    "        (loss), grads = loss_and_grad_fn(\n",
    "            model, graph.edge_index, graph.node_features, edge_weights, graph.edge_features, graph.edge_labels\n",
    "        )\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "    return avg_loss / len(train_loader)\n",
    "\n",
    "def eval():\n",
    "    avg_loss = 0.\n",
    "    for graph in eval_loader:\n",
    "        edge_weights = graph.edge_features[:, 0]\n",
    "        loss = forward_fn(\n",
    "            model, graph.edge_index, graph.node_features, edge_weights, graph.edge_features, graph.edge_labels\n",
    "        )\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "    return avg_loss / len(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During testing, the model is utilized in inference mode to calculate an anomalousness score for each edge. For demonstration purposes, we use a supervised approach to determine an optimal threshold value that distinguishes between benign and malicious edges. However, fully unsupervised methods can also be applied in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc, roc_auc_score, average_precision_score as ap_score\n",
    "\n",
    "def test():\n",
    "    all_edge_losses, all_edge_labels = [], []\n",
    "\n",
    "    for i, graph in enumerate(test_loader):\n",
    "        edge_weights = graph.edge_features[:, 0]\n",
    "        edge_losses = forward_fn(\n",
    "            model, graph.edge_index, graph.node_features, edge_weights, graph.edge_features, graph.edge_labels, inference=True\n",
    "        )\n",
    "        all_edge_losses.extend(edge_losses.tolist())\n",
    "        all_edge_labels.extend(graph.edge_labels.tolist())\n",
    "   \n",
    "    def find_best_threshold_supervised_auc(all_edge_losses, all_edge_labels):\n",
    "        all_edge_labels, all_edge_losses = np.array(all_edge_labels), np.array(all_edge_losses)\n",
    "        fpr, tpr, thresholds = roc_curve(all_edge_labels, all_edge_losses)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Filter out the points where TPR is less than 0.8\n",
    "        valid_indices = np.where(tpr >= 0.8)[0]\n",
    "        fpr_valid = fpr[valid_indices]\n",
    "        thresholds_valid = thresholds[valid_indices]\n",
    "\n",
    "        # Find the threshold corresponding to the lowest FPR among valid points\n",
    "        optimal_idx = np.argmin(fpr_valid)\n",
    "        optimal_threshold = thresholds_valid[optimal_idx]\n",
    "        \n",
    "        return optimal_threshold\n",
    "\n",
    "    def compute_metrics(ys, y_hats, all_edge_losses):\n",
    "        ys, y_hats, all_edge_losses = np.array(ys), np.array(y_hats), np.array(all_edge_losses)\n",
    "        attack_idxs = (ys == 1).nonzero()[0]\n",
    "\n",
    "        overall_acc = (ys == y_hats).mean()\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(ys, y_hats, average='binary')\n",
    "\n",
    "        tp  = y_hats[ys == 1].sum()\n",
    "        fp  = y_hats[ys == 0].sum()\n",
    "        tpr = y_hats[ys == 1].mean()\n",
    "        fpr = y_hats[ys == 0].mean()\n",
    "\n",
    "        len_positives = len((ys == 1).nonzero()[0])\n",
    "        len_negatives = len((ys == 0).nonzero()[0])\n",
    "\n",
    "        try:\n",
    "            auc_score = roc_auc_score(ys, all_edge_losses)\n",
    "            auc_fpr, auc_tpr, _ = roc_curve(ys, all_edge_losses)\n",
    "        except: auc_score = float('nan'); auc_fpr = float('nan'); auc_tpr = float('nan')\n",
    "\n",
    "        try:\n",
    "            ap = ap_score(ys, all_edge_losses)\n",
    "        except: ap = float('nan')\n",
    "\n",
    "        print(f\"Detected {tp} anomalies / {len(attack_idxs)} anomalous edges ({(tp/len(attack_idxs))*100:.5f}%).\")\n",
    "        print(f\"TPR: {tpr:.3f} | FPR: {fpr:.6f} | Overall acc: {overall_acc:.6f} | AP: {ap:.3f} | AUC: {auc_score:.3f} | recall: {recall:.3f} | precision: {precision:.3f}\")\n",
    "        print(f\"TP: {tp}/{len_positives} | FP: {fp}/{len_negatives}\\n\")\n",
    "\n",
    "        return auc_fpr, auc_tpr, auc_score\n",
    "    \n",
    "    best_threshold = find_best_threshold_supervised_auc(all_edge_losses, all_edge_labels)\n",
    "    y_hat = mx.where(mx.array(all_edge_losses) > best_threshold, 1, 0)\n",
    "\n",
    "    return compute_metrics(all_edge_labels, y_hat, all_edge_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the training loop and measure some metrics such as AUC, FPR and TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    train_loss = train()\n",
    "    eval_loss = eval()\n",
    "\n",
    "    print(f\"Epoch {epoch:>3} | Train loss: {train_loss:.4f} | Val loss: {eval_loss:.4f}\")\n",
    "    \n",
    "    auc_fpr, auc_tpr, auc_score = test()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(auc_fpr, auc_tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.5f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig('roc_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have seen how to perform anomaly detection on large graphs with the LANL dataset.\n",
    "This example provides a simple implementation to deal with the LANL dataset. We refer to the following papers for those seeking to reach state-of-the-art results with GNNs and temporal methods on this dataset. \n",
    "\n",
    "[1] Euler: Detecting Network Lateral Movement via Scalable Temporal Link Prediction\n",
    "\n",
    "[2] Understanding and Bridging the Gap Between Unsupervised Network Representation Learning and Security Analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_contribute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
