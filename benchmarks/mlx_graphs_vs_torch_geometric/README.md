# Benchmarking mlx-graphs vs torch-geometric

Benchmarks are generated by measuring the runtime of some `mlx-graphs` layers, along with their equivalent in [torch_geometric](https://github.com/pyg-team/pytorch_geometric) (PyG) with `mps` and `cpu` backends. For each layer, we measure the runtime of multiple experiments. We propose 2 benchmarks based on these experiments:

* [Detailed benchmark](results/average_benchmark.md): provides the runtime of each experiment.
* [Average runtime benchmark](results/detailed_benchmark.md): computes the mean of experiments. Easier to navigate, with fewer details.

A comprehensive benchmark of core `mlx` operations vs `torch` operations is also available [here]((https://github.com/TristanBilot/mlx-benchmark)).


## Run the benchmark locally

Install dependencies:

```python
pip install torch torch_geometric tqdm
```

Run the benchmark:
```python
python launcher.py
```
