{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification with Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** This first tutorial will go through an example of graph classification using mini-batching. \n",
    "\n",
    "**Concepts:** `Mini-batching`, `Readout`, `GNN training`, `MLX syntax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import mlx.core as mx\n",
    "from mlx_graphs.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this first tutorial, we will use the [TUDatasets](https://chrsmrrs.github.io/datasets/docs/datasets/) collection, which comprises more than 120 datasets for graph classification and graph regression tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets proposed in this collection can be easily accessed via the `TUDataset` class.\n",
    "\n",
    "We will use here the `MUTAG` dataset, where input graphs represent chemical compounds, with vertices symbolizing atoms identified by their atom type through one-hot encoding. Edges between vertices denote the bonds connecting the atoms. The dataset comprises 188 samples of chemical compounds, featuring 7 distinct node labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCI-H23H(num_graphs=40353)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TUDataset(\"NCI-H23H\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset properties can directly accessed from the `dataset` object, and we can also compute some statistics to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes\n",
      "--------------------\n",
      "Number of graphs: 40353\n",
      "Number of node features: 66\n",
      "Number of edge features: 3\n",
      "Number of graph features: 0\n",
      "Number of graph classes to predict: 2\n",
      "\n",
      "Dataset stats\n",
      "--------------------\n",
      "Mean node degree: 2.09\n",
      "Mean num of nodes: 46.67\n",
      "Mean num of edges: 97.40\n"
     ]
    }
   ],
   "source": [
    "# Some useful properties\n",
    "print(\"Dataset attributes\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of graph features: {dataset.num_graph_features}\")\n",
    "print(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n",
    "\n",
    "# Statistics of the dataset\n",
    "stats = defaultdict(list)\n",
    "for g in dataset:\n",
    "    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n",
    "    stats[\"Mean num of nodes\"].append(g.num_nodes)\n",
    "    stats[\"Mean num of edges\"].append(g.num_edges)\n",
    "\n",
    "print(\"Dataset stats\")\n",
    "print(\"-\" * 20)\n",
    "for k, v in stats.items():\n",
    "    mean = mx.mean(mx.array(v)).item()\n",
    "    print(f\"{k}: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` is nothing more than a wrapper around a list of `GraphData` objects. In **mlx-graphs**, a `GraphData` object contains the structure along with features of a graph, similarly as [DGLGraph](https://docs.dgl.ai/en/2.0.x/api/python/dgl.DGLGraph.html#dgl.DGLGraph) in DGL or [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html) in PyG.\n",
    "\n",
    "We can directly access these graphs from the dataset using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 430), int32)\n",
       "\tnode_features(shape=(207, 66), float32)\n",
       "\tedge_features(shape=(430, 3), float32)\n",
       "\tgraph_labels(shape=(1,), int32))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph of this dataset comprises 38 edges with 4 edge features and 17 nodes with 7 node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When indexing a dataset with sequences or slices, we end up with another `Dataset` object containing the graphs associated with this sequence. Using this indexing strategy, the dataset can be divided into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: NCI-H23H(num_graphs=35000)\n",
      "Testing dataset: NCI-H23H(num_graphs=5353)\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = dataset[:35000]\n",
    "# test_dataset = dataset[35000:]\n",
    "train_dataset = dataset[:35000]\n",
    "test_dataset = dataset[35000:]\n",
    "\n",
    "print(f\"Training dataset: {train_dataset}\")\n",
    "print(f\"Testing dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_graphs.loaders import Dataloader\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = Dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn\n",
    "from mlx_graphs.nn import GCNConv, global_mean_pool, Linear\n",
    "from time import time\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def __call__(self, edge_index, node_features, batch_indices):\n",
    "        \n",
    "        h = nn.relu(self.conv1(edge_index, node_features))\n",
    "        h = nn.relu(self.conv2(edge_index, h))\n",
    "        h = self.conv3(edge_index, h)\n",
    "        \n",
    "        h = global_mean_pool(h, batch_indices)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(42)\n",
    "\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    return mx.mean(nn.losses.cross_entropy(y_hat, y))\n",
    "\n",
    "def eval_fn(y_hat, y):\n",
    "    return mx.mean(mx.argmax(y_hat, axis=1) == y)\n",
    "\n",
    "def forward_fn(model, graph, labels):\n",
    "    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3767232894897461\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "a = None\n",
    "for g in train_loader:\n",
    "    a = g.node_features\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.set_default_device(mx.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.981129791692365\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "model = GCN(\n",
    "    in_dim=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    out_dim=dataset.num_graph_classes,\n",
    ")\n",
    "\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=0.01)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "def train(train_loader):\n",
    "    loss_sum = 0.0\n",
    "    for graph in train_loader:\n",
    "        \n",
    "        (loss, y_hat), grads = loss_and_grad_fn(\n",
    "            model=model,\n",
    "            graph=graph,\n",
    "            labels=graph.graph_labels,\n",
    "        )\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    acc = 0.0\n",
    "    for graph in loader:\n",
    "        y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        acc += (y_hat == graph.graph_labels).sum().item()\n",
    "    \n",
    "    return acc / len(loader.dataset)\n",
    "\n",
    "\n",
    "def epoch():\n",
    "    loss = train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "\n",
    "times = timeit.Timer(lambda: epoch()).repeat(repeat=10, number=10)\n",
    "print(min(times)/10)\n",
    "\n",
    "# for epoch in range(170):\n",
    "#     loss = train(train_loader)\n",
    "#     train_acc = test(train_loader)\n",
    "#     test_acc = test(test_loader)\n",
    "\n",
    "#     print(\n",
    "#         \" | \".join(\n",
    "#             [\n",
    "#                 f\"Epoch: {epoch:3d}\",\n",
    "#                 f\"Train loss: {loss:.3f}\",\n",
    "#                 f\"Train acc: {train_acc:.3f}\",\n",
    "#                 f\"Test acc: {test_acc:.3f}\",\n",
    "#             ]\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
