# Benchmarking mlx-graphs

Benchmarks are generated by measuring the runtime of some `mlx` layers, along with their equivalent in [torch_geometric](https://github.com/pyg-team/pytorch_geometric) (PyG) with `mps`, `cpu` backends. For each layer, we measure the runtime of multiple experiments. We propose 2 benchmarks based on these experiments:

* [Detailed benchmark](results/average_benchmark.md): provides the runtime of each experiment.
* [Average runtime benchmark](results/detailed_benchmark.md): computes the mean of experiments. Easier to navigate, with fewer details.

A comprehensive benchmark of core `mlx` operations is also available [here](Inspired from [mlx-benchmark](https://github.com/TristanBilot/mlx-benchmark)).


## Run the benchmark locally

Install dependencies:

```python
pip install torch torch_geometric tqdm
```

Run the benchmark:
```python
python run_benchmark.py
```
