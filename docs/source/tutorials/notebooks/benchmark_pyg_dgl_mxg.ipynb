{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking mlx-graphs\n",
    "\n",
    "In this notebook, we'll be benchmarking [mlx-graphs](https://github.com/mlx-graphs/mlx-graphs) (mxg) with [PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric) (PyG) and [Deep Graph Library](https://github.com/dmlc/dgl/) (DGL) on a graph classification problem\n",
    "\n",
    "We'll start by implementing the same architecture and training loop in the three frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running, the following extra dependencies should be installed:\n",
    "\n",
    "`pip install torch torch_geometric dgl pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlx-graphs (MXG) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as mlx_nn\n",
    "import mlx.optimizers as mlx_optim\n",
    "\n",
    "import mlx_graphs.nn as mxg_nn\n",
    "import mlx_graphs.datasets as mxg_datasets\n",
    "import mlx_graphs.loaders as mxg_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class MXG_model(mlx_nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(MXG_model, self).__init__()\n",
    "\n",
    "        self.conv1 = layer(in_dim, hidden_dim)\n",
    "        self.conv2 = layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = layer(hidden_dim, hidden_dim)\n",
    "        self.linear = mxg_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = mlx_nn.Dropout(p=dropout)\n",
    "\n",
    "    def __call__(self, edge_index, node_features, batch_indices):\n",
    "        h = mlx_nn.relu(self.conv1(edge_index, node_features))\n",
    "        h = mlx_nn.relu(self.conv2(edge_index, h))\n",
    "        h = self.conv3(edge_index, h)\n",
    "        \n",
    "        h = mxg_nn.global_mean_pool(h, batch_indices)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    return mlx_nn.losses.cross_entropy(y_hat, y, reduction=\"mean\")\n",
    "\n",
    "def forward_fn(model, graph):\n",
    "    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "    labels = graph.graph_labels \n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat\n",
    "\n",
    "def setup_training_mxg(dataset, layer, batch_size, hid_size):\n",
    "    loader = mxg_loaders.Dataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MXG_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset.num_node_features,\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_graph_classes,\n",
    "    )\n",
    "    mx.eval(model.parameters())\n",
    "\n",
    "    optimizer = mlx_optim.Adam(learning_rate=0.01)\n",
    "    loss_and_grad_fn = mlx_nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "    state = [model.state, optimizer.state, mx.random.state]\n",
    "\n",
    "    def step(graph):\n",
    "        (loss, y_hat), grads = loss_and_grad_fn(model=model, graph=graph) \n",
    "        optimizer.update(model, grads)\n",
    "\n",
    "    return loader, step, state\n",
    "\n",
    "def train_mxg(loader, step, state=None, epochs=1):    \n",
    "    for _ in range(epochs):\n",
    "        for graph in loader:\n",
    "            step(graph)\n",
    "            mx.eval(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.datasets as pyg_datasets\n",
    "import torch_geometric.loader as pyg_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyG_model(torch.nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n",
    "        super(PyG_model, self).__init__()\n",
    "        \n",
    "        self.conv1 = layer(in_dim, hidden_dim)\n",
    "        self.conv2 = layer(hidden_dim, hidden_dim)\n",
    "        self.conv3 = layer(hidden_dim, hidden_dim)\n",
    "        self.lin = torch_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_pyg(dataset, layer, batch_size, hid_size):\n",
    "    loader = pyg_loaders.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = PyG_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset.num_node_features,\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_classes,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch_nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    def step(data):\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loader, step, None\n",
    "\n",
    "def train_pyg(loader, step, state=None, epochs=1):\n",
    "    for _ in range(epochs):\n",
    "        for data in loader:\n",
    "            step(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn.pytorch as dgl_nn\n",
    "import dgl.data as dgl_datasets\n",
    "import dgl.dataloading as dgl_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGL_model(torch_nn.Module):\n",
    "    def __init__(self, layer, in_dim, hidden_dim, out_dim):\n",
    "        super(DGL_model, self).__init__()\n",
    "\n",
    "        if \"GATConv\" in str(layer):\n",
    "            self.conv1 = layer(in_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "            self.conv2 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "            self.conv3 = layer(hidden_dim, hidden_dim, num_heads=1, allow_zero_in_degree=True)\n",
    "        else:\n",
    "            self.conv1 = layer(in_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "            self.conv2 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "            self.conv3 = layer(hidden_dim, hidden_dim, allow_zero_in_degree=True)\n",
    "\n",
    "        self.classify = torch_nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        h = F.relu(self.conv3(g, h))\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "            return self.classify(hg.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_dgl(dataset, layer, batch_size, hid_size):\n",
    "    loader = dgl_loaders.GraphDataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    model = DGL_model(\n",
    "        layer=layer,\n",
    "        in_dim=dataset[0][0].ndata[\"x\"].shape[1],\n",
    "        hidden_dim=hid_size,\n",
    "        out_dim=dataset.num_classes,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch_nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    def step(data, labels):\n",
    "        out = model(data, data.ndata['x'])\n",
    "        loss = criterion(out, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return loader, step, None\n",
    "\n",
    "def train_dgl(loader, step, state=None, epochs=1):\n",
    "    for _ in range(epochs):\n",
    "        for data, labels in loader:\n",
    "            step(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the three implementations ready, we can proceed to benchmark them. We'll do that on the DD dataset, but other ones from TUDataset can also be used.\n",
    "\n",
    "Let's first create a common config and some auxiliray functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def dgl_dataset(name):\n",
    "    pyg_dataset = pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name)\n",
    "    dgl_dataset = dgl_datasets.TUDataset(dataset_name)\n",
    "\n",
    "    for i, (pyg, dgl) in enumerate(zip(pyg_dataset, dgl_dataset.graph_lists)):\n",
    "        dgl_dataset.graph_lists[i].ndata[\"x\"] = pyg.x\n",
    "    \n",
    "    return dgl_dataset\n",
    "\n",
    "def benchmark(framework, loader, step, state):\n",
    "    train_fn = framework_to_train[framework]\n",
    "    train_fn(loader, step, state)\n",
    "\n",
    "framework_to_setup = {\n",
    "    \"mxg\": setup_training_mxg,\n",
    "    \"pyg\": setup_training_pyg,\n",
    "    \"dgl\": setup_training_dgl,\n",
    "}\n",
    "\n",
    "framework_to_train = {\n",
    "    \"mxg\": train_mxg,\n",
    "    \"pyg\": train_pyg,\n",
    "    \"dgl\": train_dgl,\n",
    "}\n",
    "\n",
    "framework_to_datasets = {\n",
    "    \"mxg\": lambda name: mxg_datasets.TUDataset(name),\n",
    "    \"pyg\": lambda name: pyg_datasets.TUDataset(f\".mlx_graphs_data/{name}\", name),\n",
    "    \"dgl\": lambda name: dgl_dataset(name)\n",
    "}\n",
    "layer_classes = {\n",
    "    \"mxg\": {\n",
    "        \"GCNConv\": mxg_nn.GCNConv,\n",
    "        \"GATConv\": mxg_nn.GATConv,\n",
    "    },\n",
    "    \"pyg\": {\n",
    "        \"GCNConv\": pyg_nn.GCNConv,\n",
    "        \"GATConv\": pyg_nn.GATConv,\n",
    "    },\n",
    "    \"dgl\": {\n",
    "        \"GCNConv\": dgl_nn.GraphConv,\n",
    "        \"GATConv\": dgl_nn.GATConv,\n",
    "    }\n",
    "}\n",
    "\n",
    "frameworks = [\"dgl\", \"pyg\", \"mxg\"]\n",
    "datasets = [\n",
    "    # \"BZR_MD\", \n",
    "    # \"MUTAG\",\n",
    "    \"DD\",\n",
    "]\n",
    "layers = [\"GCNConv\", \"GATConv\"]\n",
    "\n",
    "batch_size = 64\n",
    "hid_size = 128\n",
    "\n",
    "TIMEIT_REPEAT = 5\n",
    "TIMEIT_NUMBER = 1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mx.random.seed(42)\n",
    "dgl.seed(42)\n",
    "\n",
    "mx.set_default_device(mx.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to benchmark the training speed of the three implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DD\n",
      "==========\n",
      "dgl | GCNConv | 0.817s\n",
      "dgl | GATConv | 1.808s\n",
      "\n",
      "pyg | GCNConv | 1.447s\n",
      "pyg | GATConv | 1.960s\n",
      "\n",
      "mxg | GCNConv | 0.483s\n",
      "mxg | GATConv | 0.694s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    print(dataset_name)\n",
    "    print(\"=\" * 10)\n",
    "\n",
    "    for framework in frameworks:\n",
    "        dataset = framework_to_datasets[framework](dataset_name)\n",
    "\n",
    "        for i, layer_name in enumerate(layers):\n",
    "            layer = layer_classes[framework][layer_name]\n",
    "            loader, step, state = framework_to_setup[framework](dataset, layer, batch_size, hid_size)\n",
    "            \n",
    "            times = timeit.Timer(\n",
    "                lambda: benchmark(framework, loader, step, state)\n",
    "            ).repeat(repeat=TIMEIT_REPEAT, number=TIMEIT_NUMBER)\n",
    "\n",
    "            time = min(times) / TIMEIT_NUMBER\n",
    "\n",
    "            print(\n",
    "                \" | \".join(\n",
    "                    [\n",
    "                        f\"{framework}\",\n",
    "                        f\"{layer_name}\",\n",
    "                        f\"{time:.3f}s\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        print(\"\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
