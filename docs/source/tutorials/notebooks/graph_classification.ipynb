{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification with Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** This first tutorial will go through an example of graph classification using mini-batching. We will explore how to generate node embeddings using a Graph Convolutional Network (GCN) model, subsequently transform these embeddings through readout, and enhance efficiency by parallelizing operations via graph batching.\n",
    "\n",
    "**Concepts:** `Mini-batching`, `Readout`, `GNN training`, `MLX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import mlx.core as mx\n",
    "from mlx_graphs.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this first tutorial, we will use the [TUDatasets](https://chrsmrrs.github.io/datasets/docs/datasets/) collection, which comprises more than 120 datasets for graph classification and graph regression tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets proposed in this collection can be easily accessed via the `TUDataset` class.\n",
    "\n",
    "We will use here the `MUTAG` dataset, where input graphs represent chemical compounds, with vertices symbolizing atoms identified by their atom type through one-hot encoding. Edges between vertices denote the bonds connecting the atoms. The dataset comprises 188 samples of chemical compounds, featuring 7 distinct node labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUTAG(num_graphs=188)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TUDataset(\"MUTAG\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset properties can directly accessed from the `dataset` object, and we can also compute some statistics to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes\n",
      "--------------------\n",
      "Number of graphs: 188\n",
      "Number of node features: 7\n",
      "Number of edge features: 4\n",
      "Number of graph features: 0\n",
      "Number of graph classes to predict: 2\n",
      "\n",
      "Dataset stats\n",
      "--------------------\n",
      "Mean node degree: 2.19\n",
      "Mean num of nodes: 17.93\n",
      "Mean num of edges: 39.59\n"
     ]
    }
   ],
   "source": [
    "# Some useful properties\n",
    "print(\"Dataset attributes\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of graph features: {dataset.num_graph_features}\")\n",
    "print(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n",
    "\n",
    "# Statistics of the dataset\n",
    "stats = defaultdict(list)\n",
    "for g in dataset:\n",
    "    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n",
    "    stats[\"Mean num of nodes\"].append(g.num_nodes)\n",
    "    stats[\"Mean num of edges\"].append(g.num_edges)\n",
    "\n",
    "print(\"Dataset stats\")\n",
    "print(\"-\" * 20)\n",
    "for k, v in stats.items():\n",
    "    mean = mx.mean(mx.array(v)).item()\n",
    "    print(f\"{k}: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` is nothing more than a wrapper around a list of `GraphData` objects. In **mlx-graphs**, a `GraphData` object contains the structure along with features of a graph, similarly as [DGLGraph](https://docs.dgl.ai/en/2.0.x/api/python/dgl.DGLGraph.html#dgl.DGLGraph) in DGL or [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html) in PyG.\n",
    "\n",
    "We can directly access these graphs from the dataset using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 38), int32)\n",
       "\tnode_features(shape=(17, 7), float32)\n",
       "\tedge_features(shape=(38, 4), float32)\n",
       "\tgraph_labels(shape=(1,), int32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph of this dataset comprises 38 edges with 4 edge features and 17 nodes with 7 node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When indexing a dataset with sequences or slices, we end up with another `Dataset` object containing the graphs associated with this sequence. Using this indexing strategy, the dataset can be divided into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: MUTAG(num_graphs=150)\n",
      "Testing dataset: MUTAG(num_graphs=38)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n",
    "print(f\"Training dataset: {train_dataset}\")\n",
    "print(f\"Testing dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Dataloader` to divide the datasets into iterable batches of graphs, a technique highly recommended for its ability to enhance parallelization of operations, thereby accelerating runtime. \n",
    "\n",
    "Within each batch, all attributes are basically concatenated, allowing multiple graphs to be represented through a single array per attribute. Importantly, this maintains the independence of each graph (i.e., the graphs remain unconnected to one another). To identify and extract the original graphs from a batch, each `GraphDataBatch` includes a `batch_indices` attribute. This attribute provides a mapping for all nodes within the batch back to their respective graphs, facilitating easy retrieval of individual graph data from the batch structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph batch of size 64\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 2590), int32)\n",
      "\tnode_features(shape=(1168, 7), float32)\n",
      "\tedge_features(shape=(2590, 4), float32)\n",
      "\tgraph_labels(shape=(64,), int32))\n",
      "array([0, 0, 0, ..., 63, 63, 63], dtype=int64)\n",
      "\n",
      "Graph batch of size 64\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 2620), int32)\n",
      "\tnode_features(shape=(1179, 7), float32)\n",
      "\tedge_features(shape=(2620, 4), float32)\n",
      "\tgraph_labels(shape=(64,), int32))\n",
      "array([0, 0, 0, ..., 63, 63, 63], dtype=int64)\n",
      "\n",
      "Graph batch of size 22\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 720), int32)\n",
      "\tnode_features(shape=(337, 7), float32)\n",
      "\tedge_features(shape=(720, 4), float32)\n",
      "\tgraph_labels(shape=(22,), int32))\n",
      "array([0, 0, 0, ..., 21, 21, 21], dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from mlx_graphs.loaders import Dataloader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = Dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"\\nGraph batch of size {len(batch)}\")\n",
    "    print(batch)\n",
    "    print(batch.batch_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN model\n",
    "\n",
    "Let's define a basic 3-layer Graph Convolutional Network (GCN) using the `GCNConv` layer. It is as simple as creating a new class inheriting from `mlx.nn.Module` and implementing the `__call__` method, responsible for the forward pass.\n",
    "\n",
    "We employ `global_mean_pool`, also known as a readout operation, to compute the mean of all node embeddings, resulting in a graph embedding that we can pass as input to a final linear layer for graph classification. As we are working with batches of graphs, we need to provide the `batch_indices` of the batch to `global_mean_pool` in order to compute the pooling operation individually for each batch. The output of the pooling operation will thus be (num_batches, embedding_size), here (64, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn\n",
    "from mlx_graphs.nn import GCNConv, global_mean_pool, Linear\n",
    "import time\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def __call__(self, edge_index, node_features, batch_indices):\n",
    "        h = nn.relu(self.conv1(edge_index, node_features))\n",
    "        h = nn.relu(self.conv2(edge_index, h))\n",
    "        h = self.conv3(edge_index, h)\n",
    "        \n",
    "        h = global_mean_pool(h, batch_indices)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our GCN model in a supervised fashion using cross entropy loss. Here's how we define the loss function and the forward function in MLX. It is recommended in MLX to write a dedicated function for the forward pass as this function will later by passed to `nn.value_and_grad` in order to compute the gradients of the model w.r.t the output loss of the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    return mx.mean(nn.losses.cross_entropy(y_hat, y))\n",
    "\n",
    "def forward_fn(model, graph, labels):\n",
    "    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, MLX computations are performed on the Mac's integrated GPU, leveraging its multiple cores for efficient operations. This is the preferred method for mlx-graphs to optimize parallel processing of GNN tasks. However, you can effortlessly switch between computing on the CPU and the GPU using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = mx.gpu # or mx.cpu\n",
    "mx.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the GCN model is done similarly as in other frameworks like Jax or PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    loss_sum = 0.0\n",
    "    for graph in train_loader:\n",
    "        \n",
    "        (loss, y_hat), grads = loss_and_grad_fn(\n",
    "            model=model,\n",
    "            graph=graph,\n",
    "            labels=graph.graph_labels,\n",
    "        )\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    acc = 0.0\n",
    "    for graph in loader:\n",
    "        y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        acc += (y_hat == graph.graph_labels).sum().item()\n",
    "    \n",
    "    return acc / len(loader.dataset)\n",
    "\n",
    "def epoch():\n",
    "    loss = train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    return loss, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train loss: 0.013 | Train acc: 0.667 | Test acc: 0.684\n",
      "Epoch:   1 | Train loss: 0.014 | Train acc: 0.633 | Test acc: 0.684\n",
      "Epoch:   2 | Train loss: 0.013 | Train acc: 0.653 | Test acc: 0.684\n",
      "Epoch:   3 | Train loss: 0.013 | Train acc: 0.647 | Test acc: 0.658\n",
      "Epoch:   4 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.711\n",
      "Epoch:   5 | Train loss: 0.012 | Train acc: 0.660 | Test acc: 0.658\n",
      "Epoch:   6 | Train loss: 0.011 | Train acc: 0.713 | Test acc: 0.684\n",
      "Epoch:   7 | Train loss: 0.014 | Train acc: 0.620 | Test acc: 0.658\n",
      "Epoch:   8 | Train loss: 0.013 | Train acc: 0.687 | Test acc: 0.684\n",
      "Epoch:   9 | Train loss: 0.012 | Train acc: 0.553 | Test acc: 0.474\n",
      "Epoch:  10 | Train loss: 0.014 | Train acc: 0.667 | Test acc: 0.658\n",
      "Epoch:  11 | Train loss: 0.012 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  12 | Train loss: 0.013 | Train acc: 0.673 | Test acc: 0.605\n",
      "Epoch:  13 | Train loss: 0.012 | Train acc: 0.647 | Test acc: 0.632\n",
      "Epoch:  14 | Train loss: 0.012 | Train acc: 0.693 | Test acc: 0.711\n",
      "Epoch:  15 | Train loss: 0.012 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  16 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  17 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  18 | Train loss: 0.011 | Train acc: 0.693 | Test acc: 0.737\n",
      "Epoch:  19 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch:  20 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.711\n",
      "Epoch:  21 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.605\n",
      "Epoch:  22 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.579\n",
      "Epoch:  23 | Train loss: 0.010 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  24 | Train loss: 0.011 | Train acc: 0.713 | Test acc: 0.605\n",
      "Epoch:  25 | Train loss: 0.010 | Train acc: 0.713 | Test acc: 0.658\n",
      "Epoch:  26 | Train loss: 0.011 | Train acc: 0.693 | Test acc: 0.632\n",
      "Epoch:  27 | Train loss: 0.012 | Train acc: 0.700 | Test acc: 0.658\n",
      "Epoch:  28 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  29 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  30 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.711\n",
      "Epoch:  31 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  32 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  33 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  34 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  35 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  36 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch:  37 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  38 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  39 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.658\n",
      "Epoch:  40 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.605\n",
      "Epoch:  41 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  42 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.605\n",
      "Epoch:  43 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.579\n",
      "Epoch:  44 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  45 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  46 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  47 | Train loss: 0.010 | Train acc: 0.713 | Test acc: 0.605\n",
      "Epoch:  48 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.605\n",
      "Epoch:  49 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch:  50 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.579\n",
      "Epoch:  51 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.684\n",
      "Epoch:  52 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.605\n",
      "Epoch:  53 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.579\n",
      "Epoch:  54 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  55 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  56 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.605\n",
      "Epoch:  57 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.684\n",
      "Epoch:  58 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.579\n",
      "Epoch:  59 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.579\n",
      "Epoch:  60 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  61 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.711\n",
      "Epoch:  62 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.579\n",
      "Epoch:  63 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch:  64 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch:  65 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.579\n",
      "Epoch:  66 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  67 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.605\n",
      "Epoch:  68 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.605\n",
      "Epoch:  69 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.605\n",
      "Epoch:  70 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.684\n",
      "Epoch:  71 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  72 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.684\n",
      "Epoch:  73 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.711\n",
      "Epoch:  74 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.605\n",
      "Epoch:  75 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.605\n",
      "Epoch:  76 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.579\n",
      "Epoch:  77 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  78 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  79 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  80 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  81 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  82 | Train loss: 0.010 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  83 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  84 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  85 | Train loss: 0.011 | Train acc: 0.680 | Test acc: 0.632\n",
      "Epoch:  86 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  87 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch:  88 | Train loss: 0.012 | Train acc: 0.720 | Test acc: 0.632\n",
      "Epoch:  89 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.632\n",
      "Epoch:  90 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  91 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  92 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.579\n",
      "Epoch:  93 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.553\n",
      "Epoch:  94 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  95 | Train loss: 0.010 | Train acc: 0.713 | Test acc: 0.579\n",
      "Epoch:  96 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.605\n",
      "Epoch:  97 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  98 | Train loss: 0.012 | Train acc: 0.727 | Test acc: 0.605\n",
      "Epoch:  99 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "\n",
      "==> Best test accuracy: 0.737\n"
     ]
    }
   ],
   "source": [
    "import mlx.optimizers as optim\n",
    "mx.random.seed(42)\n",
    "\n",
    "model = GCN(\n",
    "    in_dim=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    out_dim=dataset.num_graph_classes,\n",
    ")\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=0.01)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "epochs = 100\n",
    "best_test_acc = 0.0\n",
    "for e in range(epochs):\n",
    "    loss, train_acc, test_acc = epoch()\n",
    "    best_test_acc = max(best_test_acc, test_acc)\n",
    "\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"Epoch: {e:3d}\",\n",
    "                f\"Train loss: {loss:.3f}\",\n",
    "                f\"Train acc: {train_acc:.3f}\",\n",
    "                f\"Test acc: {test_acc:.3f}\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "print(f\"\\n==> Best test accuracy: {best_test_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
