{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification with Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** This first tutorial will go through an example of graph classification using mini-batching. \n",
    "\n",
    "**Concepts:** `Mini-batching`, `Readout`, `GNN training`, `MLX syntax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import mlx.core as mx\n",
    "from mlx_graphs.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this first tutorial, we will use the [TUDatasets](https://chrsmrrs.github.io/datasets/docs/datasets/) collection, which comprises more than 120 datasets for graph classification and graph regression tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets proposed in this collection can be easily accessed via the `TUDataset` class.\n",
    "\n",
    "We will use here the `MUTAG` dataset, where input graphs represent chemical compounds, with vertices symbolizing atoms identified by their atom type through one-hot encoding. Edges between vertices denote the bonds connecting the atoms. The dataset comprises 188 samples of chemical compounds, featuring 7 distinct node labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MUTAG(num_graphs=188)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TUDataset(\"MUTAG\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset properties can directly accessed from the `dataset` object, and we can also compute some statistics to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset attributes\n",
      "--------------------\n",
      "Number of graphs: 188\n",
      "Number of node features: 7\n",
      "Number of edge features: 4\n",
      "Number of graph features: 0\n",
      "Number of graph classes to predict: 2\n",
      "\n",
      "Dataset stats\n",
      "--------------------\n",
      "Mean node degree: 2.19\n",
      "Mean num of nodes: 17.93\n",
      "Mean num of edges: 39.59\n"
     ]
    }
   ],
   "source": [
    "# Some useful properties\n",
    "print(\"Dataset attributes\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of edge features: {dataset.num_edge_features}\")\n",
    "print(f\"Number of graph features: {dataset.num_graph_features}\")\n",
    "print(f\"Number of graph classes to predict: {dataset.num_graph_classes}\\n\")\n",
    "\n",
    "# Statistics of the dataset\n",
    "stats = defaultdict(list)\n",
    "for g in dataset:\n",
    "    stats[\"Mean node degree\"].append(g.num_edges / g.num_nodes)\n",
    "    stats[\"Mean num of nodes\"].append(g.num_nodes)\n",
    "    stats[\"Mean num of edges\"].append(g.num_edges)\n",
    "\n",
    "print(\"Dataset stats\")\n",
    "print(\"-\" * 20)\n",
    "for k, v in stats.items():\n",
    "    mean = mx.mean(mx.array(v)).item()\n",
    "    print(f\"{k}: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` is nothing more than a wrapper around a list of `GraphData` objects. In **mlx-graphs**, a `GraphData` object contains the structure along with features of a graph, similarly as [DGLGraph](https://docs.dgl.ai/en/2.0.x/api/python/dgl.DGLGraph.html#dgl.DGLGraph) in DGL or [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html) in PyG.\n",
    "\n",
    "We can directly access these graphs from the dataset using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(\n",
       "\tedge_index(shape=(2, 38), int32)\n",
       "\tnode_features(shape=(17, 7), float32)\n",
       "\tedge_features(shape=(38, 4), float32)\n",
       "\tgraph_labels(shape=(1,), int32))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph of this dataset comprises 38 edges with 4 edge features and 17 nodes with 7 node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When indexing a dataset with sequences or slices, we end up with another `Dataset` object containing the graphs associated with this sequence. Using this indexing strategy, the dataset can be divided into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: MUTAG(num_graphs=150)\n",
      "Testing dataset: MUTAG(num_graphs=38)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n",
    "print(f\"Training dataset: {train_dataset}\")\n",
    "print(f\"Testing dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 2590), int32)\n",
      "\tnode_features(shape=(1168, 7), float32)\n",
      "\tedge_features(shape=(2590, 4), float32)\n",
      "\tgraph_labels(shape=(64,), int32))\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 2620), int32)\n",
      "\tnode_features(shape=(1179, 7), float32)\n",
      "\tedge_features(shape=(2620, 4), float32)\n",
      "\tgraph_labels(shape=(64,), int32))\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 22\n",
      "GraphDataBatch(\n",
      "\tedge_index(shape=(2, 720), int32)\n",
      "\tnode_features(shape=(337, 7), float32)\n",
      "\tedge_features(shape=(720, 4), float32)\n",
      "\tgraph_labels(shape=(22,), int32))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_graphs.loaders import Dataloader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = Dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.nn as nn\n",
    "from mlx_graphs.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def __call__(self, edge_index, node_features, batch_indices):\n",
    "        h = nn.relu(self.conv1(edge_index, node_features))\n",
    "        h = nn.relu(self.conv2(edge_index, h))\n",
    "        h = self.conv3(edge_index, h)\n",
    "\n",
    "        h = global_mean_pool(h, batch_indices)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(42)\n",
    "\n",
    "def loss_fn(y_hat, y, parameters=None):\n",
    "    return mx.mean(nn.losses.cross_entropy(y_hat, y))\n",
    "\n",
    "def eval_fn(y_hat, y):\n",
    "    return mx.mean(mx.argmax(y_hat, axis=1) == y)\n",
    "\n",
    "def forward_fn(model, graph, labels):\n",
    "    y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "    loss = loss_fn(y_hat, labels, model.parameters())\n",
    "    return loss, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(\n",
      "    (linear): Linear(input_dims=7, output_dims=64, bias=True)\n",
      "  )\n",
      "  (conv2): GCNConv(\n",
      "    (linear): Linear(input_dims=64, output_dims=64, bias=True)\n",
      "  )\n",
      "  (conv3): GCNConv(\n",
      "    (linear): Linear(input_dims=64, output_dims=64, bias=True)\n",
      "  )\n",
      "  (lin): Linear(input_dims=64, output_dims=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Epoch:   0 | Train loss: 0.015 | Train acc: 0.580 | Test acc: 0.526\n",
      "Epoch:   1 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:   2 | Train loss: 0.012 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:   3 | Train loss: 0.013 | Train acc: 0.647 | Test acc: 0.684\n",
      "Epoch:   4 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:   5 | Train loss: 0.013 | Train acc: 0.653 | Test acc: 0.658\n",
      "Epoch:   6 | Train loss: 0.014 | Train acc: 0.620 | Test acc: 0.684\n",
      "Epoch:   7 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:   8 | Train loss: 0.014 | Train acc: 0.667 | Test acc: 0.684\n",
      "Epoch:   9 | Train loss: 0.014 | Train acc: 0.640 | Test acc: 0.658\n",
      "Epoch:  10 | Train loss: 0.013 | Train acc: 0.653 | Test acc: 0.658\n",
      "Epoch:  11 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  12 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  13 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  14 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  15 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  16 | Train loss: 0.014 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  17 | Train loss: 0.013 | Train acc: 0.620 | Test acc: 0.684\n",
      "Epoch:  18 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  19 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  20 | Train loss: 0.012 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  21 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  22 | Train loss: 0.012 | Train acc: 0.673 | Test acc: 0.632\n",
      "Epoch:  23 | Train loss: 0.013 | Train acc: 0.660 | Test acc: 0.684\n",
      "Epoch:  24 | Train loss: 0.011 | Train acc: 0.667 | Test acc: 0.684\n",
      "Epoch:  25 | Train loss: 0.011 | Train acc: 0.660 | Test acc: 0.711\n",
      "Epoch:  26 | Train loss: 0.012 | Train acc: 0.693 | Test acc: 0.737\n",
      "Epoch:  27 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.658\n",
      "Epoch:  28 | Train loss: 0.012 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch:  29 | Train loss: 0.011 | Train acc: 0.647 | Test acc: 0.553\n",
      "Epoch:  30 | Train loss: 0.012 | Train acc: 0.733 | Test acc: 0.605\n",
      "Epoch:  31 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.684\n",
      "Epoch:  32 | Train loss: 0.012 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  33 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  34 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  35 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch:  36 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  37 | Train loss: 0.010 | Train acc: 0.700 | Test acc: 0.658\n",
      "Epoch:  38 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch:  39 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch:  40 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch:  41 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch:  42 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch:  43 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch:  44 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.684\n",
      "Epoch:  45 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch:  46 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.684\n",
      "Epoch:  47 | Train loss: 0.012 | Train acc: 0.753 | Test acc: 0.605\n",
      "Epoch:  48 | Train loss: 0.011 | Train acc: 0.700 | Test acc: 0.658\n",
      "Epoch:  49 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  50 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  51 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  52 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  53 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.737\n",
      "Epoch:  54 | Train loss: 0.011 | Train acc: 0.713 | Test acc: 0.711\n",
      "Epoch:  55 | Train loss: 0.012 | Train acc: 0.753 | Test acc: 0.658\n",
      "Epoch:  56 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  57 | Train loss: 0.012 | Train acc: 0.720 | Test acc: 0.632\n",
      "Epoch:  58 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  59 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  60 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  61 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.658\n",
      "Epoch:  62 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  63 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.684\n",
      "Epoch:  64 | Train loss: 0.011 | Train acc: 0.707 | Test acc: 0.684\n",
      "Epoch:  65 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  66 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.605\n",
      "Epoch:  67 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch:  68 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.684\n",
      "Epoch:  69 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.684\n",
      "Epoch:  70 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.684\n",
      "Epoch:  71 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.711\n",
      "Epoch:  72 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch:  73 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.711\n",
      "Epoch:  74 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch:  75 | Train loss: 0.010 | Train acc: 0.713 | Test acc: 0.658\n",
      "Epoch:  76 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  77 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  78 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.684\n",
      "Epoch:  79 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch:  80 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch:  81 | Train loss: 0.011 | Train acc: 0.733 | Test acc: 0.684\n",
      "Epoch:  82 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.658\n",
      "Epoch:  83 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  84 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  85 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.711\n",
      "Epoch:  86 | Train loss: 0.010 | Train acc: 0.720 | Test acc: 0.632\n",
      "Epoch:  87 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  88 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch:  89 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.658\n",
      "Epoch:  90 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.632\n",
      "Epoch:  91 | Train loss: 0.011 | Train acc: 0.720 | Test acc: 0.658\n",
      "Epoch:  92 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch:  93 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch:  94 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  95 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.684\n",
      "Epoch:  96 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch:  97 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch:  98 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch:  99 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch: 100 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.711\n",
      "Epoch: 101 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.684\n",
      "Epoch: 102 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.684\n",
      "Epoch: 103 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch: 104 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch: 105 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch: 106 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch: 107 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch: 108 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch: 109 | Train loss: 0.011 | Train acc: 0.727 | Test acc: 0.632\n",
      "Epoch: 110 | Train loss: 0.010 | Train acc: 0.727 | Test acc: 0.711\n",
      "Epoch: 111 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.658\n",
      "Epoch: 112 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch: 113 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.632\n",
      "Epoch: 114 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.605\n",
      "Epoch: 115 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch: 116 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.684\n",
      "Epoch: 117 | Train loss: 0.010 | Train acc: 0.773 | Test acc: 0.605\n",
      "Epoch: 118 | Train loss: 0.009 | Train acc: 0.767 | Test acc: 0.632\n",
      "Epoch: 119 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.632\n",
      "Epoch: 120 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch: 121 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch: 122 | Train loss: 0.010 | Train acc: 0.733 | Test acc: 0.684\n",
      "Epoch: 123 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch: 124 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch: 125 | Train loss: 0.010 | Train acc: 0.740 | Test acc: 0.632\n",
      "Epoch: 126 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.684\n",
      "Epoch: 127 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch: 128 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.711\n",
      "Epoch: 129 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.658\n",
      "Epoch: 130 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 131 | Train loss: 0.011 | Train acc: 0.740 | Test acc: 0.684\n",
      "Epoch: 132 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.684\n",
      "Epoch: 133 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 134 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.632\n",
      "Epoch: 135 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.684\n",
      "Epoch: 136 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.632\n",
      "Epoch: 137 | Train loss: 0.009 | Train acc: 0.740 | Test acc: 0.658\n",
      "Epoch: 138 | Train loss: 0.012 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch: 139 | Train loss: 0.011 | Train acc: 0.747 | Test acc: 0.684\n",
      "Epoch: 140 | Train loss: 0.011 | Train acc: 0.753 | Test acc: 0.684\n",
      "Epoch: 141 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch: 142 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 143 | Train loss: 0.010 | Train acc: 0.780 | Test acc: 0.684\n",
      "Epoch: 144 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.632\n",
      "Epoch: 145 | Train loss: 0.010 | Train acc: 0.773 | Test acc: 0.684\n",
      "Epoch: 146 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.684\n",
      "Epoch: 147 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 148 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch: 149 | Train loss: 0.010 | Train acc: 0.780 | Test acc: 0.711\n",
      "Epoch: 150 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.711\n",
      "Epoch: 151 | Train loss: 0.010 | Train acc: 0.760 | Test acc: 0.632\n",
      "Epoch: 152 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch: 153 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 154 | Train loss: 0.011 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch: 155 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.658\n",
      "Epoch: 156 | Train loss: 0.010 | Train acc: 0.773 | Test acc: 0.632\n",
      "Epoch: 157 | Train loss: 0.010 | Train acc: 0.780 | Test acc: 0.684\n",
      "Epoch: 158 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.605\n",
      "Epoch: 159 | Train loss: 0.010 | Train acc: 0.773 | Test acc: 0.632\n",
      "Epoch: 160 | Train loss: 0.010 | Train acc: 0.747 | Test acc: 0.632\n",
      "Epoch: 161 | Train loss: 0.010 | Train acc: 0.753 | Test acc: 0.711\n",
      "Epoch: 162 | Train loss: 0.010 | Train acc: 0.780 | Test acc: 0.684\n",
      "Epoch: 163 | Train loss: 0.010 | Train acc: 0.773 | Test acc: 0.658\n",
      "Epoch: 164 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.658\n",
      "Epoch: 165 | Train loss: 0.011 | Train acc: 0.760 | Test acc: 0.658\n",
      "Epoch: 166 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.684\n",
      "Epoch: 167 | Train loss: 0.011 | Train acc: 0.787 | Test acc: 0.684\n",
      "Epoch: 168 | Train loss: 0.010 | Train acc: 0.780 | Test acc: 0.684\n",
      "Epoch: 169 | Train loss: 0.010 | Train acc: 0.767 | Test acc: 0.658\n"
     ]
    }
   ],
   "source": [
    "import mlx.optimizers as optim\n",
    "\n",
    "model = GCN(\n",
    "    in_dim=dataset.num_node_features,\n",
    "    hidden_dim=64,\n",
    "    out_dim=dataset.num_graph_classes,\n",
    ")\n",
    "print(model)\n",
    "\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=0.01)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, forward_fn)\n",
    "\n",
    "def train(train_loader):\n",
    "    loss_sum = 0.0\n",
    "    for graph in train_loader:\n",
    "        (loss, y_hat), grads = loss_and_grad_fn(\n",
    "            model=model,\n",
    "            graph=graph,\n",
    "            labels=graph.graph_labels,\n",
    "        )\n",
    "        optimizer.update(model, grads)\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "        loss_sum += loss.item()\n",
    "    return loss_sum / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    acc = 0.0\n",
    "    for graph in loader:\n",
    "        y_hat = model(graph.edge_index, graph.node_features, graph.batch_indices)\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        acc += (y_hat == graph.graph_labels).sum().item()\n",
    "    \n",
    "    return acc / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(170):\n",
    "\n",
    "    loss = train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "\n",
    "    print(\n",
    "        \" | \".join(\n",
    "            [\n",
    "                f\"Epoch: {epoch:3d}\",\n",
    "                f\"Train loss: {loss:.3f}\",\n",
    "                f\"Train acc: {train_acc:.3f}\",\n",
    "                f\"Test acc: {test_acc:.3f}\",\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_contribute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
